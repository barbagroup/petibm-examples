************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

petibm-ibpm on a linux-gnu-openmpi-opt named phantom with 4 processors, by mesnardo Mon Oct 21 17:25:12 2019
Using Petsc Release Version 3.11.2, May, 18, 2019 

                         Max       Max/Min     Avg       Total 
Time (sec):           2.745e+03     1.000   2.745e+03
Objects:              6.315e+03     1.004   6.297e+03
Flop:                 3.258e+11     1.002   3.254e+11  1.302e+12
Flop/sec:             1.187e+08     1.002   1.186e+08  4.742e+08
MPI Messages:         1.486e+05     1.303   1.239e+05  4.955e+05
MPI Message Lengths:  2.680e+10     2.900   1.100e+05  5.452e+10
MPI Reductions:       8.850e+04     1.000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flop
                            and VecAXPY() for complex vectors of length N --> 8N flop

Summary of Stages:   ----- Time ------  ----- Flop ------  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total    Count   %Total     Avg         %Total    Count   %Total 
 0:      Main Stage: 2.2181e+00   0.1%  0.0000e+00   0.0%  0.000e+00   0.0%  0.000e+00        0.0%  2.100e+01   0.0% 
 1:      initialize: 3.7722e+00   0.1%  4.1033e+07   0.0%  5.045e+02   0.1%  1.442e+05        0.1%  3.380e+02   0.4% 
 2:     rhsVelocity: 1.7552e+02   6.4%  2.0972e+11  16.1%  1.410e+05  28.5%  3.292e+03        0.9%  0.000e+00   0.0% 
 3:   solveVelocity: 5.5615e+02  20.3%  1.0306e+12  79.2%  2.160e+05  43.6%  5.261e+03        2.1%  3.900e+04  44.1% 
 4:      rhsPoisson: 2.4855e+01   0.9%  2.6310e+10   2.0%  7.800e+04  15.7%  2.308e+03        0.3%  6.000e+03   6.8% 
 5:    solvePoisson: 1.9526e+03  71.1%  0.0000e+00   0.0%  2.700e+04   5.4%  1.948e+06       96.4%  0.000e+00   0.0% 
 6:          update: 2.8119e+01   1.0%  3.5064e+10   2.7%  3.300e+04   6.7%  2.585e+03        0.2%  0.000e+00   0.0% 
 7:           write: 1.0680e+00   0.0%  0.0000e+00   0.0%  0.000e+00   0.0%  0.000e+00        0.0%  1.000e+01   0.0% 
 8:         monitor: 4.2746e-04   0.0%  0.0000e+00   0.0%  0.000e+00   0.0%  0.000e+00        0.0%  0.000e+00   0.0% 
 9: integrateForces: 3.7559e-01   0.0%  0.0000e+00   0.0%  0.000e+00   0.0%  0.000e+00        0.0%  9.003e+03  10.2% 

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flop: Max - maximum over all processors
                  Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   AvgLen: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flop in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flop over all processors)/(max time over all processors)
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flop                              --- Global ---  --- Stage ----  Total
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   AvgLen  Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s
------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

VecView                1 1.0 2.8999e-02 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   1  0  0  0  0     0
VecSet                 1 1.0 2.4320e-06 1.6 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0

--- Event Stage 1: initialize

BuildTwoSided          6 1.0 9.7163e-03 3.3 0.00e+00 0.0 6.5e+00 4.0e+00 0.0e+00  0  0  0  0  0   0  0  1  0  0     0
BuildTwoSidedF        20 1.0 2.1432e-01 1.9 0.00e+00 0.0 1.5e+01 1.5e+04 0.0e+00  0  0  0  0  0   4  0  3  0  0     0
MatConvert             4 1.0 4.3536e-01 1.0 0.00e+00 0.0 4.4e+01 1.0e+03 3.2e+01  0  0  0  0  0  11  0  9  0  9     0
MatScale               4 1.0 1.6425e-02 1.1 4.38e+06 1.0 2.0e+01 4.4e+03 0.0e+00  0  0  0  0  0   0 43  4  0  0  1065
MatAssemblyBegin      30 1.0 2.1499e-01 1.9 0.00e+00 0.0 1.5e+01 1.5e+04 0.0e+00  0  0  0  0  0   4  0  3  0  0     0
MatAssemblyEnd        30 1.0 4.3615e-01 1.1 0.00e+00 0.0 2.7e+02 8.8e+02 1.4e+02  0  0  0  0  0  11  0 53  0 41     0
MatGetRowIJ            1 1.0 3.9901e-07 1.7 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatCreateSubMat        1 1.0 2.7355e-01 1.0 0.00e+00 0.0 1.5e+01 3.3e+06 1.6e+01  0  0  0  0  0   7  0  3 69  5     0
MatTranspose           1 1.0 3.2834e-02 1.0 0.00e+00 0.0 6.0e+01 4.6e+03 1.0e+01  0  0  0  0  0   1  0 12  0  3     0
MatMatMult             2 1.0 2.2737e-01 1.0 5.89e+06 1.0 9.0e+01 7.4e+03 2.6e+01  0  0  0  0  0   6 57 18  1  8   103
MatMatMultSym          2 1.0 1.5756e-01 1.0 0.00e+00 0.0 7.9e+01 5.5e+03 2.4e+01  0  0  0  0  0   4  0 16  1  7     0
MatMatMultNum          2 1.0 5.9375e-02 1.0 5.89e+06 1.0 1.1e+01 2.1e+04 0.0e+00  0  0  0  0  0   2 57  2  0  0   396
MatGetLocalMat         5 1.0 1.1229e-01 1.8 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   2  0  0  0  0     0
MatGetBrAoCol          4 1.0 1.8517e-03 1.1 0.00e+00 0.0 4.4e+01 1.4e+04 0.0e+00  0  0  0  0  0   0  0  9  1  0     0
MatTranspose_SeqAIJ_FAST       1 1.0 1.2619e-03 1.5 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
SFSetGraph             6 1.0 3.6251e-06 1.4 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
SFSetUp                6 1.0 3.4882e-02 1.3 0.00e+00 0.0 2.0e+01 1.1e+03 0.0e+00  0  0  0  0  0   1  0  4  0  0     0
SFReduceBegin         10 1.0 1.0064e-02 1.1 0.00e+00 0.0 1.3e+01 1.7e+03 0.0e+00  0  0  0  0  0   0  0  3  0  0     0
SFReduceEnd           10 1.0 9.5363e-03 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecView               15 0.0 2.8904e-01 0.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   2  0  0  0  0     0
VecSet                30 1.0 9.8917e-03 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecScatterBegin        4 1.3 1.4832e-0319.3 0.00e+00 0.0 3.2e+01 5.0e+03 0.0e+00  0  0  0  0  0   0  0  6  0  0     0
VecScatterEnd          3 1.0 2.6771e-04 3.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0

--- Event Stage 2: rhsVelocity

BuildTwoSidedF      6000 1.0 4.9102e-01 1.4 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatMult            15000 1.0 9.7082e+01 1.0 2.92e+10 1.0 1.4e+05 3.3e+03 0.0e+00  4  9 28  1  0  55 56100100  0  1200
MatMultAdd          3000 1.0 1.3659e+01 1.0 2.92e+09 1.0 0.0e+00 0.0e+00 0.0e+00  0  1  0  0  0   8  6  0  0  0   853
VecScale           21000 1.0 1.8862e+01 1.1 5.83e+09 1.0 0.0e+00 0.0e+00 0.0e+00  1  2  0  0  0  10 11  0  0  0  1236
VecSet              6000 1.0 8.4400e+00 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   5  0  0  0  0     0
VecAXPY            18000 1.0 5.0278e+01 1.0 1.75e+10 1.0 0.0e+00 0.0e+00 0.0e+00  2  5  0  0  0  28 33  0  0  0  1391
VecSwap             3000 1.0 1.0154e+01 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   6  0  0  0  0     0
VecAssemblyBegin    6000 1.0 5.6824e-01 1.4 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecAssemblyEnd      6000 1.0 1.5433e-02 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecScatterBegin    12000 1.0 9.0094e+00 1.1 0.00e+00 0.0 1.4e+05 3.3e+03 0.0e+00  0  0 28  1  0   5  0100100  0     0
VecScatterEnd      12000 1.0 5.4037e+00 2.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   3  0  0  0  0     0

--- Event Stage 3: solveVelocity

MatMult            18000 1.0 2.7000e+02 1.0 1.49e+11 1.0 2.2e+05 5.3e+03 0.0e+00 10 46 44  2  0  48 58100100  0  2198
VecDot             18000 1.0 3.3922e+01 1.0 1.75e+10 1.0 0.0e+00 0.0e+00 1.8e+04  1  5  0  0 20   6  7  0  0 46  2061
VecDotNorm2         9000 1.0 1.7029e+01 1.0 1.75e+10 1.0 0.0e+00 0.0e+00 9.0e+03  1  5  0  0 10   3  7  0  0 23  4106
VecNorm            12000 1.0 1.4050e+01 1.1 1.17e+10 1.0 0.0e+00 0.0e+00 1.2e+04  0  4  0  0 14   2  5  0  0 31  3318
VecCopy             6000 1.0 1.5786e+01 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   3  0  0  0  0     0
VecSet              9001 1.0 1.4749e+01 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   3  0  0  0  0     0
VecAXPBYCZ         18000 1.0 6.0766e+01 1.0 3.50e+10 1.0 0.0e+00 0.0e+00 0.0e+00  2 11  0  0  0  11 14  0  0  0  2302
VecWAXPY           18000 1.0 6.1308e+01 1.0 1.75e+10 1.0 0.0e+00 0.0e+00 0.0e+00  2  5  0  0  0  11  7  0  0  0  1141
VecPointwiseMult   21000 1.0 7.1284e+01 1.0 1.02e+10 1.0 0.0e+00 0.0e+00 0.0e+00  3  3  0  0  0  13  4  0  0  0   572
VecScatterBegin    18000 1.0 1.3463e+00 1.5 0.00e+00 0.0 2.2e+05 5.3e+03 0.0e+00  0  0 44  2  0   0  0100100  0     0
VecScatterEnd      18000 1.0 1.6434e+01 6.8 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   2  0  0  0  0     0
KSPSetUp               1 1.0 1.0655e-02 1.5 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
KSPSolve            3000 1.0 5.5616e+02 1.0 2.58e+11 1.0 2.2e+05 5.3e+03 3.9e+04 20 79 44  2 44 100100100100100  1853
PCSetUp                1 1.0 7.4599e-07 1.8 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
PCApply            21000 1.0 7.1361e+01 1.0 1.02e+10 1.0 0.0e+00 0.0e+00 2.0e+00  3  3  0  0  0  13  4  0  0  0   572

--- Event Stage 4: rhsPoisson

BuildTwoSidedF      6000 1.0 1.2503e+00 3.8 0.00e+00 0.0 4.5e+04 7.2e+00 0.0e+00  0  0  9  0  0   2  0 58  0  0     0
MatMult             6000 1.0 1.8774e+01 1.0 5.12e+09 1.0 5.6e+04 3.2e+03 0.0e+00  1  2 11  0  0  75 78 71100  0  1091
MatMultAdd          3000 1.0 8.7911e+00 1.0 1.46e+09 1.0 2.2e+04 7.2e+00 0.0e+00  0  0  5  0  0  35 22 29  0  0   664
VecScale            3000 1.0 4.4753e-04 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecSet              3000 1.0 2.3861e+00 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0  10  0  0  0  0     0
VecAXPY             3000 1.0 4.1580e+00 1.0 1.46e+09 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0  16 22  0  0  0  1403
VecAssemblyBegin    6000 1.0 1.3388e+00 1.1 0.00e+00 0.0 4.5e+04 7.2e+00 0.0e+00  0  0  9  0  0   5  0 58  0  0     0
VecAssemblyEnd      6000 1.0 1.9096e-02 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecScatterBegin     3000 1.0 2.1804e-01 4.0 0.00e+00 0.0 3.3e+04 5.4e+03 0.0e+00  0  0  7  0  0   0  0 42100  0     0
VecScatterEnd       3000 1.0 4.6896e-01 1.5 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   2  0  0  0  0     0

--- Event Stage 5: solvePoisson

VecScatterBegin     9000 1.0 1.5813e+01 2.0 0.00e+00 0.0 2.7e+04 1.9e+06 0.0e+00  0  0  5 96  0   1  0100100  0     0
VecScatterEnd       9000 1.0 1.6653e+01 2.7 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   1  0  0  0  0     0

--- Event Stage 6: update

MatMult             3000 1.0 2.0886e+01 1.2 4.40e+09 1.0 3.3e+04 2.6e+03 0.0e+00  1  1  7  0  0  68 50100100  0   841
VecAXPY             6000 1.0 9.3940e+00 1.1 4.38e+09 1.0 0.0e+00 0.0e+00 0.0e+00  0  1  0  0  0  32 50  0  0  0  1863
VecScatterBegin     3000 1.0 1.7093e-01 1.7 0.00e+00 0.0 3.3e+04 2.6e+03 0.0e+00  0  0  7  0  0   0  0100100  0     0
VecScatterEnd       3000 1.0 2.8531e+0013.4 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   5  0  0  0  0     0

--- Event Stage 7: write

VecView                9 1.0 9.6162e-01 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0  90  0  0  0  0     0

--- Event Stage 8: monitor


--- Event Stage 9: integrateForces

------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

    Distributed Mesh     2              6        32052     0.
              Matrix     0             25    252057632     0.
           Index Set     2             10      2924588     0.
   IS L to G Mapping     1              5      4904688     0.
   Star Forest Graph     4             12        10080     0.
     Discrete System     2              6         5520     0.
              Vector     5             38     91639320     0.
         Vec Scatter     1             12      9746832     0.
   Application Order     0              3     23318944     0.
              Viewer     3              4         3376     0.
       Krylov Solver     0              1         1488     0.
      Preconditioner     0              1          864     0.

--- Event Stage 1: initialize

    Distributed Mesh     4              0            0     0.
              Matrix    66             41    556301436     0.
           Index Set    66             58     34154952     0.
   IS L to G Mapping     6              2      1961560     0.
   Star Forest Graph    14              6         5424     0.
     Discrete System     4              0            0     0.
              Vector    84             60     31331576     0.
         Vec Scatter    25             14        19184     0.
   Application Order     3              0            0     0.
              Viewer     7              5         4160     0.
       Krylov Solver     1              0            0     0.
      Preconditioner     1              0            0     0.

--- Event Stage 2: rhsVelocity

              Vector     1              0            0     0.

--- Event Stage 3: solveVelocity

              Vector     7              0            0     0.

--- Event Stage 4: rhsPoisson

              Vector  3001           3000   5838240000     0.

--- Event Stage 5: solvePoisson


--- Event Stage 6: update


--- Event Stage 7: write

              Viewer     5              5         4160     0.

--- Event Stage 8: monitor


--- Event Stage 9: integrateForces

              Vector  3000           3000     14520000     0.
========================================================================================================================
Average time to get PetscTime(): 3.27011e-08
Average time for MPI_Barrier(): 1.9612e-06
Average time for zero size MPI_Send(): 1.64074e-06
#PETSc Option Table entries:
-log_view ascii:view.log
-options_left
-velocity_ksp_atol 1e-6
-velocity_ksp_max_it 1000
-velocity_ksp_rtol 0.0
-velocity_ksp_type bcgs
-velocity_pc_jacobi_type diagonal
-velocity_pc_type jacobi
#End of PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: --PETSC_DIR=/home/mesnardo/software/petsc/3.11.2 --PETSC_ARCH=linux-gnu-openmpi-opt --with-debugging=0 COPTFLAGS=-O3 CXXOPTFLAGS=-O3 FOPTFLAGS=-O3 --with-cxx-dialect=C++11 --with-shared-libraries=1 --with-pic=1 --with-mpi-dir=/home/mesnardo/software/openmpi/3.0.4/linux-gnu-opt --with-cuda-dir=/usr/local/cuda-10.1 --download-hdf5 --download-fblaslapack --download-hypre --download-ptscotch --download-metis --download-parmetis --download-superlu_dist
-----------------------------------------
Libraries compiled on 2019-06-18 19:59:51 on phantom 
Machine characteristics: Linux-4.4.0-127-generic-x86_64-with-debian-stretch-sid
Using PETSc directory: /home/mesnardo/software/petsc/3.11.2
Using PETSc arch: linux-gnu-openmpi-opt
-----------------------------------------

Using C compiler: /home/mesnardo/software/openmpi/3.0.4/linux-gnu-opt/bin/mpicc  -fPIC -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fstack-protector -fvisibility=hidden -O3  
Using Fortran compiler: /home/mesnardo/software/openmpi/3.0.4/linux-gnu-opt/bin/mpif90  -fPIC -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -O3    
-----------------------------------------

Using include paths: -I/home/mesnardo/software/petsc/3.11.2/include -I/home/mesnardo/software/petsc/3.11.2/linux-gnu-openmpi-opt/include -I/usr/local/cuda-10.1/include -I/home/mesnardo/software/openmpi/3.0.4/linux-gnu-opt/include
-----------------------------------------

Using C linker: /home/mesnardo/software/openmpi/3.0.4/linux-gnu-opt/bin/mpicc
Using Fortran linker: /home/mesnardo/software/openmpi/3.0.4/linux-gnu-opt/bin/mpif90
Using libraries: -Wl,-rpath,/home/mesnardo/software/petsc/3.11.2/linux-gnu-openmpi-opt/lib -L/home/mesnardo/software/petsc/3.11.2/linux-gnu-openmpi-opt/lib -lpetsc -Wl,-rpath,/home/mesnardo/software/petsc/3.11.2/linux-gnu-openmpi-opt/lib -L/home/mesnardo/software/petsc/3.11.2/linux-gnu-openmpi-opt/lib -Wl,-rpath,/usr/local/cuda-10.1/lib64 -L/usr/local/cuda-10.1/lib64 -Wl,-rpath,/home/mesnardo/software/openmpi/3.0.4/linux-gnu-opt/lib -L/home/mesnardo/software/openmpi/3.0.4/linux-gnu-opt/lib -Wl,-rpath,/usr/lib/gcc/x86_64-linux-gnu/5 -L/usr/lib/gcc/x86_64-linux-gnu/5 -Wl,-rpath,/usr/lib/x86_64-linux-gnu -L/usr/lib/x86_64-linux-gnu -Wl,-rpath,/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -lHYPRE -lsuperlu_dist -lflapack -lfblas -lhdf5hl_fortran -lhdf5_fortran -lhdf5_hl -lhdf5 -lparmetis -lmetis -lptesmumps -lptscotchparmetis -lptscotch -lptscotcherr -lesmumps -lscotch -lscotcherr -lm -lX11 -lcufft -lcublas -lcudart -lcusparse -lstdc++ -ldl -lmpi_usempif08 -lmpi_usempi_ignore_tkr -lmpi_mpifh -lmpi -lgfortran -lm -lgfortran -lm -lgcc_s -lquadmath -lpthread -lrt -lm -lpthread -lz -lstdc++ -ldl
-----------------------------------------

